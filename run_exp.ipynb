{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from dopamine.agents.rainbow import rainbow_agent\n",
    "from dopamine.discrete_domains import run_experiment, gym_lib\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from absl import flags\n",
    "import gin.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # chosen achieve reasonable performance.\n",
      "    import dopamine.agents.rainbow.rainbow_agent\n",
      "    import dopamine.discrete_domains.gym_lib\n",
      "    import dopamine.discrete_domains.run_experiment\n",
      "    import dopamine.replay_memory.prioritized_replay_buffer\n",
      "    import gin.tf.external_configurables\n",
      "\n",
      "    RainbowAgent.observation_shape = %gym_lib.ABR_OBSERVATION_SHAPE\n",
      "    RainbowAgent.observation_dtype = %gym_lib.ABR_OBSERVATION_DTYPE\n",
      "    RainbowAgent.stack_size = %gym_lib.ABR_STACK_SIZE\n",
      "    RainbowAgent.network = @gym_lib.ABRRainbowNetwork\n",
      "    RainbowAgent.num_atoms = 51\n",
      "    RainbowAgent.vmax = 10.\n",
      "    RainbowAgent.gamma = 0.99\n",
      "    RainbowAgent.update_horizon = 3\n",
      "    RainbowAgent.min_replay_history = 10\n",
      "    RainbowAgent.update_period = 100\n",
      "    RainbowAgent.target_update_period = 100\n",
      "    RainbowAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
      "    RainbowAgent.replay_scheme = 'prioritized'\n",
      "    # RainbowAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
      "    RainbowAgent.tf_device = '/cpu:*'\n",
      "    RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
      "\n",
      "    tf.train.AdamOptimizer.learning_rate = 0.09\n",
      "    tf.train.AdamOptimizer.epsilon = 0.0003125\n",
      "\n",
      "    # TODO: figure out how to make the environment with gym make\n",
      "    create_gym_environment.environment_name = 'ABR'\n",
      "    create_gym_environment.version = 'v1'\n",
      "    create_agent.agent_name = 'rainbow'\n",
      "    Runner.create_environment_fn = @gym_lib.create_gym_environment\n",
      "    Runner.num_iterations = 2\n",
      "    Runner.training_steps = 2000\n",
      "    Runner.evaluation_steps = 1000\n",
      "    Runner.max_steps_per_episode = 500\n",
      "\n",
      "    WrappedPrioritizedReplayBuffer.replay_capacity = 50000\n",
      "    WrappedPrioritizedReplayBuffer.batch_size = 128\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def gen_config(min_replay_history, update_period):\n",
    "    config = \"\"\"\n",
    "    # chosen achieve reasonable performance.\n",
    "    import dopamine.agents.rainbow.rainbow_agent\n",
    "    import dopamine.discrete_domains.gym_lib\n",
    "    import dopamine.discrete_domains.run_experiment\n",
    "    import dopamine.replay_memory.prioritized_replay_buffer\n",
    "    import gin.tf.external_configurables\n",
    "\n",
    "    RainbowAgent.observation_shape = %gym_lib.ABR_OBSERVATION_SHAPE\n",
    "    RainbowAgent.observation_dtype = %gym_lib.ABR_OBSERVATION_DTYPE\n",
    "    RainbowAgent.stack_size = %gym_lib.ABR_STACK_SIZE\n",
    "    RainbowAgent.network = @gym_lib.ABRRainbowNetwork\n",
    "    RainbowAgent.num_atoms = 51\n",
    "    RainbowAgent.vmax = 10.\n",
    "    RainbowAgent.gamma = 0.99\n",
    "    RainbowAgent.update_horizon = 3\n",
    "    RainbowAgent.min_replay_history = {}\n",
    "    RainbowAgent.update_period = {}\n",
    "    RainbowAgent.target_update_period = 100\n",
    "    RainbowAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
    "    RainbowAgent.replay_scheme = 'prioritized'\n",
    "    # RainbowAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
    "    RainbowAgent.tf_device = '/cpu:*'\n",
    "    RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
    "\n",
    "    tf.train.AdamOptimizer.learning_rate = 0.09\n",
    "    tf.train.AdamOptimizer.epsilon = 0.0003125\n",
    "\n",
    "    # TODO: figure out how to make the environment with gym make\n",
    "    create_gym_environment.environment_name = 'ABR'\n",
    "    create_gym_environment.version = 'v1'\n",
    "    create_agent.agent_name = 'rainbow'\n",
    "    Runner.create_environment_fn = @gym_lib.create_gym_environment\n",
    "    Runner.num_iterations = 2\n",
    "    Runner.training_steps = 2000\n",
    "    Runner.evaluation_steps = 1000\n",
    "    Runner.max_steps_per_episode = 500\n",
    "\n",
    "    WrappedPrioritizedReplayBuffer.replay_capacity = 50000\n",
    "    WrappedPrioritizedReplayBuffer.batch_size = 128\n",
    "    \"\"\".format(min_replay_history, update_period)\n",
    "    return config\n",
    "print(gen_config(10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating TrainRunner ...\n",
      "INFO:tensorflow:Creating RainbowAgent agent with the following parameters:\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "INFO:tensorflow:\t update_horizon: 3.000000\n",
      "INFO:tensorflow:\t min_replay_history: 500\n",
      "INFO:tensorflow:\t update_period: 4\n",
      "INFO:tensorflow:\t target_update_period: 100\n",
      "INFO:tensorflow:\t epsilon_train: 0.010000\n",
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n",
      "INFO:tensorflow:\t epsilon_decay_period: 250000\n",
      "INFO:tensorflow:\t tf_device: /cpu:*\n",
      "INFO:tensorflow:\t use_staging: True\n",
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x144000e90>\n",
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n",
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n",
      "INFO:tensorflow:\t observation_shape: (11, 1)\n",
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.float32'>\n",
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n",
      "INFO:tensorflow:\t stack_size: 1\n",
      "INFO:tensorflow:\t replay_capacity: 50000\n",
      "INFO:tensorflow:\t batch_size: 128\n",
      "INFO:tensorflow:\t update_horizon: 3\n",
      "INFO:tensorflow:\t gamma: 0.990000\n",
      "WARNING:tensorflow:From /Users/harrisonbrown/Documents/research_ml/dopamine/dopamine/replay_memory/circular_replay_buffer.py:821: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbrown/Documents/research_ml/dopamine/dopenv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/harrisonbrown/Documents/research_ml/dopamine/dopenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/harrisonbrown/Documents/research_ml/dopamine/dopamine/agents/rainbow/rainbow_agent.py:230: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /Users/harrisonbrown/Documents/research_ml/dopamine/dopamine/agents/rainbow/rainbow_agent.py:261: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /Users/harrisonbrown/Documents/research_ml/dopamine/dopamine/agents/dqn/dqn_agent.py:206: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "INFO:tensorflow:legacy_checkpoint_load: False\n"
     ]
    }
   ],
   "source": [
    "gin.parse_config(config, skip_unknown=False)\n",
    "LOG_PATH = \"test_remove\"\n",
    "\n",
    "def create_agent(sess, environment, summary_writer=None):\n",
    "    return rainbow_agent.RainbowAgent(sess, num_actions=environment.action_space.n)\n",
    "\n",
    "rainbow_runner = run_experiment.TrainRunner(LOG_PATH, create_agent, create_environment_fn = gym_lib.create_gym_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train agent, please be patient, may be a while...\n",
      "INFO:tensorflow:Beginning training...\n",
      "INFO:tensorflow:Average undiscounted return per training episode: -637.23\n",
      "INFO:tensorflow:Average training steps per second: 200.60\n",
      "INFO:tensorflow:Average undiscounted return per training episode: 258.99\n",
      "INFO:tensorflow:Average training steps per second: 170.51\n",
      "Done training!\n"
     ]
    }
   ],
   "source": [
    "print('Will train agent, please be patient, may be a while...')\n",
    "rainbow_runner.run_experiment()\n",
    "print('Done training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOOOO\n",
      "Reading statistics from: test_remove//logs/log_1\n",
      "YOoo dict_keys([])\n",
      "YOoo dict_keys(['train_episode_lengths', 'train_episode_returns', 'train_average_return'])\n",
      "YOoo dict_keys(['train_episode_lengths', 'train_episode_returns', 'train_average_return'])\n",
      "YOoo dict_keys(['train_episode_lengths', 'train_episode_returns', 'train_average_return'])\n",
      "YOoo dict_keys(['train_episode_lengths', 'train_episode_returns', 'train_average_return'])\n"
     ]
    }
   ],
   "source": [
    "data = colab_utils.read_experiment(\n",
    "    LOG_PATH, verbose=True, summary_keys=['train_episode_returns', 'train_average_return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    258.992821\n",
      "Name: train_episode_returns, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data['agent'] = 'yoyo'\n",
    "data['run_number'] = 1\n",
    "print(data.loc[data['iteration'] == 1]['train_episode_returns'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopenv",
   "language": "python",
   "name": "dopenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
