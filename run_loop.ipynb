{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    # chosen achieve reasonable performance.\n",
      "    import dopamine.agents.rainbow.rainbow_agent\n",
      "    import dopamine.discrete_domains.gym_lib\n",
      "    import dopamine.discrete_domains.run_experiment\n",
      "    import dopamine.replay_memory.prioritized_replay_buffer\n",
      "    import gin.tf.external_configurables\n",
      "\n",
      "    RainbowAgent.observation_shape = %gym_lib.ABR_OBSERVATION_SHAPE\n",
      "    RainbowAgent.observation_dtype = %gym_lib.ABR_OBSERVATION_DTYPE\n",
      "    RainbowAgent.stack_size = %gym_lib.ABR_STACK_SIZE\n",
      "    RainbowAgent.network = @gym_lib.ABRRainbowNetwork\n",
      "    RainbowAgent.num_atoms = 51\n",
      "    RainbowAgent.vmax = 10.\n",
      "    RainbowAgent.gamma = 0.99\n",
      "    RainbowAgent.update_horizon = 3\n",
      "    RainbowAgent.min_replay_history = 10\n",
      "    RainbowAgent.update_period = 100\n",
      "    RainbowAgent.target_update_period = 10\n",
      "    RainbowAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
      "    RainbowAgent.replay_scheme = 'prioritized'\n",
      "    # RainbowAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
      "    RainbowAgent.tf_device = '/cpu:*'\n",
      "    RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
      "\n",
      "    tf.train.AdamOptimizer.learning_rate = 0.09\n",
      "    tf.train.AdamOptimizer.epsilon = 0.0003125\n",
      "\n",
      "    # TODO: figure out how to make the environment with gym make\n",
      "    create_gym_environment.environment_name = 'ABR'\n",
      "    create_gym_environment.version = 'v1'\n",
      "    create_agent.agent_name = 'rainbow'\n",
      "    Runner.create_environment_fn = @gym_lib.create_gym_environment\n",
      "    Runner.num_iterations = 2\n",
      "    Runner.training_steps = 2000\n",
      "    Runner.evaluation_steps = 1000\n",
      "    Runner.max_steps_per_episode = 500\n",
      "\n",
      "    WrappedPrioritizedReplayBuffer.replay_capacity = 50000\n",
      "    WrappedPrioritizedReplayBuffer.batch_size = 128\n",
      "    \n",
      "INFO:tensorflow:Creating TrainRunner ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating TrainRunner ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating RainbowAgent agent with the following parameters:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbrown/Documents/research_ml/dopamine/dopenv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "INFO:tensorflow:Creating RainbowAgent agent with the following parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t min_replay_history: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t min_replay_history: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_period: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_period: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t target_update_period: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t target_update_period: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_train: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_train: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_decay_period: 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_decay_period: 250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t tf_device: /cpu:*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t tf_device: /cpu:*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t use_staging: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t use_staging: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x14fad5050>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x14fad5050>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_shape: (11, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_shape: (11, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t stack_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t stack_size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t replay_capacity: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t replay_capacity: 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t batch_size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t batch_size: 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:legacy_checkpoint_load: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:legacy_checkpoint_load: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Beginning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Beginning training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: 79.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: 79.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 55.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 55.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: 129.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: 129.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 44.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 44.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOOOO\n",
      "Reading statistics from: test/500_1//logs/log_1\n",
      "INFO:tensorflow:Creating TrainRunner ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating TrainRunner ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating RainbowAgent agent with the following parameters:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonbrown/Documents/research_ml/dopamine/dopenv/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "INFO:tensorflow:Creating RainbowAgent agent with the following parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t min_replay_history: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t min_replay_history: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_period: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_period: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t target_update_period: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t target_update_period: 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_train: 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_train: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_eval: 0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_decay_period: 250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t epsilon_decay_period: 250000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t tf_device: /cpu:*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t tf_device: /cpu:*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t use_staging: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t use_staging: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x153d23d10>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t optimizer: <tensorflow.python.training.adam.AdamOptimizer object at 0x153d23d10>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t max_tf_checkpoints_to_keep: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_shape: (11, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_shape: (11, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t observation_dtype: <class 'numpy.float32'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t terminal_dtype: <class 'numpy.uint8'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t stack_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t stack_size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t replay_capacity: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t replay_capacity: 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t batch_size: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t batch_size: 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t update_horizon: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\t gamma: 0.990000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:legacy_checkpoint_load: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:legacy_checkpoint_load: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Beginning training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Beginning training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: -1174.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: -1174.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 97.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 97.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: -80.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average undiscounted return per training episode: -80.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 88.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average training steps per second: 88.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOOOO\n",
      "Reading statistics from: test/500_2//logs/log_1\n",
      "[129.38473646321302, -80.88809793099513]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from dopamine.agents.rainbow import rainbow_agent\n",
    "from dopamine.discrete_domains import run_experiment, gym_lib\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from absl import flags\n",
    "import gin.tf\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path, exist_ok = True)\n",
    "        \n",
    "def gen_config(min_replay_history, update_period, target_update_period, num_iterations, training_steps, evaluation_steps):\n",
    "    config = \"\"\"\n",
    "    # chosen achieve reasonable performance.\n",
    "    import dopamine.agents.rainbow.rainbow_agent\n",
    "    import dopamine.discrete_domains.gym_lib\n",
    "    import dopamine.discrete_domains.run_experiment\n",
    "    import dopamine.replay_memory.prioritized_replay_buffer\n",
    "    import gin.tf.external_configurables\n",
    "\n",
    "    RainbowAgent.observation_shape = %gym_lib.ABR_OBSERVATION_SHAPE\n",
    "    RainbowAgent.observation_dtype = %gym_lib.ABR_OBSERVATION_DTYPE\n",
    "    RainbowAgent.stack_size = %gym_lib.ABR_STACK_SIZE\n",
    "    RainbowAgent.network = @gym_lib.ABRRainbowNetwork\n",
    "    RainbowAgent.num_atoms = 51\n",
    "    RainbowAgent.vmax = 10.\n",
    "    RainbowAgent.gamma = 0.99\n",
    "    RainbowAgent.update_horizon = 3\n",
    "    RainbowAgent.min_replay_history = {}\n",
    "    RainbowAgent.update_period = {}\n",
    "    RainbowAgent.target_update_period = {}\n",
    "    RainbowAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
    "    RainbowAgent.replay_scheme = 'prioritized'\n",
    "    # RainbowAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
    "    RainbowAgent.tf_device = '/cpu:*'\n",
    "    RainbowAgent.optimizer = @tf.train.AdamOptimizer()\n",
    "\n",
    "    tf.train.AdamOptimizer.learning_rate = 0.09\n",
    "    tf.train.AdamOptimizer.epsilon = 0.0003125\n",
    "\n",
    "    # TODO: figure out how to make the environment with gym make\n",
    "    create_gym_environment.environment_name = 'ABR'\n",
    "    create_gym_environment.version = 'v1'\n",
    "    create_agent.agent_name = 'rainbow'\n",
    "    Runner.create_environment_fn = @gym_lib.create_gym_environment\n",
    "    Runner.num_iterations = {}\n",
    "    Runner.training_steps = {}\n",
    "    Runner.evaluation_steps = {}\n",
    "    Runner.max_steps_per_episode = 500\n",
    "\n",
    "    WrappedPrioritizedReplayBuffer.replay_capacity = 50000\n",
    "    WrappedPrioritizedReplayBuffer.batch_size = 128\n",
    "    \"\"\".format(min_replay_history, update_period, target_update_period, num_iterations, training_steps, evaluation_steps)\n",
    "    return config\n",
    "print(gen_config(10, 100, 10, 2, 2000, 1000))\n",
    "\n",
    "# min_replay_histories = [500, 5000, 20000]\n",
    "# update_periods = [1, 2, 4]\n",
    "# target_update_periods = [500, 1000]\n",
    "# num_training_steps = 2000\n",
    "# evaluation_steps = 1000\n",
    "# num_iterations = 2\n",
    "\n",
    "# DIR = \"test/\"\n",
    "# create_folder_if_not_exists(DIR)\n",
    "\n",
    "# min_replay_histories = [500]\n",
    "# update_periods = [1, 2]\n",
    "# target_update_periods = [500]\n",
    "# num_training_steps = 2000\n",
    "# evaluation_steps = 1000\n",
    "# num_iterations = 2\n",
    "\n",
    "DIR = \"grid_1/\"\n",
    "create_folder_if_not_exists(DIR)\n",
    "\n",
    "min_replay_histories = [500, 5000, 20000]\n",
    "update_periods = [1, 4]\n",
    "target_update_periods = [50, 100, 1000, 4000, 80000]\n",
    "num_training_steps = 100000\n",
    "evaluation_steps = 200000\n",
    "num_iterations = 10\n",
    "\n",
    "all_end = []\n",
    "for mrh in min_replay_histories:\n",
    "    for up in update_periods:\n",
    "        for tup in target_update_periods:\n",
    "            kwargs = {\"min_replay_history\":mrh, \"update_period\":up, \n",
    "                                \"training_steps\": num_training_steps, \n",
    "                                \"evaluation_steps\": evaluation_steps, \"target_update_period\": tup, \n",
    "                                 \"num_iterations\": num_iterations}\n",
    "            config = gen_config(**kwargs)\n",
    "            gin.parse_config(config, skip_unknown=False)\n",
    "            LOG_PATH =  DIR + str(mrh) + \"_\" + str(up)\n",
    "            def create_agent(sess, environment, summary_writer=None):\n",
    "                return rainbow_agent.RainbowAgent(sess, num_actions=environment.action_space.n)\n",
    "            rainbow_runner = run_experiment.TrainRunner(LOG_PATH, create_agent, create_environment_fn = gym_lib.create_gym_environment)\n",
    "            rainbow_runner.run_experiment()\n",
    "            data = colab_utils.read_experiment(\n",
    "            LOG_PATH, verbose=True, summary_keys=['train_episode_returns', 'train_average_return'])\n",
    "            final_eval = data.loc[data['iteration'] == num_iterations - 1]['train_episode_returns'][1]\n",
    "            all_end.append(final_eval)\n",
    "            with open(DIR + \"aresfile.txt\", \"a\") as myfile:\n",
    "                myfile.write(\"average score: \" + str(final_eval) + \"   MODEL: \" + str(kwargs) + '\\n')\n",
    "print(all_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(yo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopenv",
   "language": "python",
   "name": "dopenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
